---
title: "State space modelling tutorial"
author: "Author: Matthew J. Crossley"
date: "Last update: `r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: false
    toc_float:
        collapsed: true
        smooth_scroll: true
    toc_depth: 4
    fig_caption: yes
    number_sections: false
    theme: cerulean
fontsize: 14pt
---

```{python, engine.path = '/Users/mq20185996/miniconda3/bin/python'}

# Import things that we will make use of
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.cm as cm
from scipy.optimize import differential_evolution
from scipy.optimize import LinearConstraint

```

- Probably the best place to begin is to write some code
  that will simulate a model of interest and plot the
  results in an intelligible way.

```{python}

# Define the parameters of the model. Ultimately, we want
# to write this code as a function that can be passed to a
# search algorithm in order to find the parameters that
# best fit our data, but as a start, just getting the model
# to run with a fixed set of parameters will be
# instructive.
eta_mu = 0.5
eta_sig = 0.5
b0_mu = 0.5
b0_sig = 0.5
b1 = 0.5
mu_init = 0.25
sig_init = 0.25

# This model requires that we know what rotation was
# applied on each trial (this determines the centroid of
# the midpoint cloud). When we get the optimisation phase
# (i.e., passing a function to an algorithm to search for
# best fitting parameters), we willl have to read in the
# rotation from a file that corresponds what a subject or
# group of subjects actually experienced. Here, since we
# are just simulatnig willy nilly, we just make something
# up that roughly corresponds to what our participants
# experienced.
rot = np.concatenate(( np.zeros(25),
                     5*np.random.random(50) + 5,
                     np.zeros(25) ))

```

- `np.zeros(n)` creates an array of zeros with `n` elements

- `np.random.random(n)` creates an array of uniformly random
  numbers in the half-open interval [0.0, 1.0).

- Note that the `np.` in the above lines refers to the
  `numpy` library, which we named `np` when we imported it
  with the line `import numpy as np `.

- When we use this kind of syntax, we are basically telling
  python to use the requested function inside the `numpy`
  library.

- Similarly, `np.random.random(n)` is just telling python to
  use the `random` function inside of the `random` division
  of the numpy library (libraries are free to divide their
  content up however they wish).

```{python}

# Define num_trials from the rotation array defined above
num_trials = rot.shape[0]

```

- Here, `shape` **is not a function**. Rather, it is an
  **attribute**.

- Basically, every numpy array stores its own dimensions in
  the attribute `shape`, and the syntax `rot.shape` tells
  python to grab the numpy array `rot`, and pull the `shape`
  attribute from it.

- The `shape` attribute is a **tuple**, so `rot.shape[0]` is
  grabbing the first element of that tuple.

```{python}

# Specifiy the number of targets. On each trial, a single
# target will be reached to, and we will want to pull this
# information from the actual data files once we move on to
# fitting. For now, in familiar willy nilly form, we just
# make something up that looks kinda right.
theta_values = np.array([-45, -30, -15, 0, 15, 30, 45])

# Pick the training target
theta_train_ind = np.where(theta_values == 0)[0][0]

# construct an array that indicates what target was reached
# to on each trial. Will need to read this in from a data
# file when fitting, but this is another instance of willy
# nilly for now.
theta_ind = theta_train_ind * np.ones(num_trials, dtype=np.int8)

# Comute the number of targets from theta_ind. The number
# of targets is important to compute becuase it corresponds
# to the number states that our state-space model will
# model across trials.
n_tgt = np.unique(theta_values).shape[0]

```

- Here, we use the numpy function `np.unique()`, which as
  you probably expect, returns the unique elements from an
  array.

- `np.unique(theta_ind).shape[0]` uses **chaining** to
  retrieve the first element of the shape attribute of the
  array that results from the ``np.unique(theta_ind)` call.

```{python}

# This model and paradigm require that we specify the mean
# (i.e., centroid) and uncertainty of midpoint feedback.
# For full model fitting awesomeness we will need to read
# these in from behavioural data files, but for now just go
# willy nilly.
mu_f = 0.5 * np.ones((n_tgt, num_trials))
sig_f = 0.5 * np.ones((n_tgt, num_trials))

# Create an array of zeros to store the simulated prior
# mean and prior uncertainty.
mu_p = np.zeros((n_tgt, num_trials))
sig_p = np.zeros((n_tgt, num_trials))

```

- We have seen `np.zeros` before, but this is the first time
  we have seen it in this code with a tuple passed in as the
  argument.

- In general, `np.zeros(n, m)` creates an array of zeros
  that has shape `(n, m)`, which corresponds to having `n`
  rows and `m` columns. This generalises to higher
  dimensions in the expected way.

```{python}

# initialise the prior mean and prior uncertainty.
mu_p[:, 0] = mu_init
sig_p[:, 0] = sig_init

# create an array of zeros that we will end up populating
# with the experienced error gradients (it's not practical
# to explain here, so just consult the paper for what the
# heck an error gradient is in this model).
delta_mu = np.zeros(num_trials)
delta_sig = np.zeros(num_trials)

```

- That marks the end of what is more or less prep work, and
  we are now ready to actually simulate the model.

```{python}

# Define a function to compute how much learning from
# the current reach will generalise to the remaining
# (not reached to) targets.
def g_func(theta_g, theta_r, b0, b1):
    '''
    theta_g should be a numpy array containing the taraget locations
    theta_r should be the scalar-valued current target location
    b0 is the parameter by the same name in the paper
    b1 is the parameter by the same name in the paper
    '''
    alpha = (b0 + np.exp(b1))
    return b0 + np.exp(b1 * np.cos(np.deg2rad(theta_g - theta_r))) / alpha

# inspect the shape of the generalisation function
g = g_func(theta_values, theta_values[theta_ind[0]], 0.5, 0.5)

fig, ax = plt.subplots(nrows=1, ncols=1)
ax.plot(theta_values, g)
ax.set_xticks(theta_values)
plt.show()

```

- The important thing to bear in mind when trying to parse
  the `g_func()` function is that any arithmetic operation
  of a scalar with a numpy array will by default apply that
  arithmetic operation to each element of the array and
  return the array. It essentially behaves like matlab in
  this regard.

- With this in mind, you should be able to see that this
  function computes how much a reach to target `theta_r`
  generalises to all other targets `theta_g`, including
  itself.

- Representing it this way allows us to handle
  generalisation by simply multiplying our state space
  vectors (see the last line of the simulation code below)
  by a generalisation vector.

```{python}

for i in range(0, num_trials-1):
    ind = theta_ind[i]

    # Virtually unintelligble, yet correct. Please consult the paper
    sp = sig_p[ind, i]**2 / (sig_p[ind, i]**2 + sig_f[ind, i]**2)
    sf = sig_f[ind, i]**2 / (sig_p[ind, i]**2 + sig_f[ind, i]**2)**2

    # Virtually unintelligble, yet correct. Please consult the paper
    delta_sig[i] = 2 * (rot[i] - mu_p[ind, i] + sp *
                        (mu_p[ind, i] - mu_f[ind, i])) * (
                            mu_p[ind, i] -
                            mu_f[ind, i]) * sf * 2 * sig_p[ind, i]

    # Virtually unintelligble, yet correct. Please consult the paper
    delta_mu[i] = -2 * (mu_p[ind, i] - (1 - sp) * mu_p[ind, i] -
                        sp * mu_f[ind, i]) * (1 - sp)

    # compute how much learning from the current reach will
    # generalise to the remaining (not reached to) targets.
    G_sig = g_func(theta_values, theta_values[theta_ind[i]], b0_sig, b1)
    G_mu = g_func(theta_values, theta_values[theta_ind[i]], b0_mu, b1)

    # Update the prior mean and prior uncertainty
    sig_p[:, i + 1] = sig_p[:, i] - eta_sig * delta_sig[i] * G_sig
    mu_p[:, i + 1] = mu_p[:, i] - eta_mu * delta_mu[i] * G_mu

```

- `range(n, m)` returns a sequence of numbers starting a `1`
  and ending at `m-1`. Recall that the index of the last
  element of a collection of objects in python is given by
  `m-1` (because python indexing is zero-based), which can
  be an annoying source of bugs for habitual matlab and R
  users.

```{python}

fig, ax = plt.subplots(nrows=1, ncols=2)
c = cm.rainbow(np.linspace(0, 1, 11))

for k in range(mu_p.shape[0]):
    ax[0].plot(mu_p[k, :], '.', color=c[k])
    ax[0].plot(rot, 'k')

for k in range(sig_p.shape[0]):
    ax[1].plot(sig_p[k, :], '.', color=c[k])
    ax[1].plot(rot, 'k')

plt.show()

```

- Well, we obviously have some debugging to do, but I think
  the structure is basically there.

- After getting this simple example to simulate and provide
  reasonable looking results, we need to move on to writing
  a function or use with the search for best fitting
  parameter values.

- To do so, we will use the `differential_evolution()`
  function the `scipy.optimize` library.

- One thing that python tends to do very well is create a
  standard interface for working with functions like
  `differential_evolution()`.

- To use it, we need to write a function that takes as its
  arguments some collection of free parameters `p`, and some
  collection of fixed parameters `args`, and returns as a
  result some scalar measure of **the goodness of fit**.

- Then define the function you want to optimise as
  `obj_func(params, *args)`.

- To get the goodness of fit, we need to compare what we
  observed in our experiment (i.e., our actual data) to what
  the model predicts given some particular parameter values.

- Because of this, it is often quite convenient to write two
  additional functions, say `simulate(paarams, *args)` and
  `get_observed()` to generate and return the predicted
  results and the observed results, respectively.

- When writing the `simulate(paarams, *args)` and
  `get_observed()` functions, we need to be careful to
  ensure that they return results in the same format. This
  will allow us to compute how well the predicted results
  match the observed results by using the sum of squared
  error (SSE) difference between them.

- That is, SSE is a measure of the goodness of fit, and it
  is this quantity that we will construct `obf_func()`
  return, and therefore, it is this quantity that we will
  have `differntial_evolution()` attempt to minimise by
  finding the best fitting parameters.

```{python}

def obj_func(params, *args):
    '''
    params: collection of free paraameter values
    *arags: fixed parameters et al.
    '''
    x_pred = simulate(params, args)
    x_obs = get_observed()
    sse = np.sum(x_pred**2 - x_obs**2) # NOTE: Check this syntax
    return sse

def simulate(params, *args):
    pass

def get_observed():
    pass

```

- Once these function is defined, we ask
  `differential_evolution()` to find us best fits as
  follows.

```{python, eval=FALSE}

constraints = LinearConstraint(A=[[1, 0, 0, -1, 0, 0, 0, 0],
                                    [0, 1, 0, 0, -1, 0, 0, 0],
                                    [0, 0, 0, 0, 0, 0, 0, 0],
                                    [0, 0, 0, 0, 0, 0, 0, 0],
                                    [0, 0, 0, 0, 0, 0, 0, 0],
                                    [0, 0, 0, 0, 0, 0, 0, 0],
                                    [0, 0, 0, 0, 0, 0, 0, 0],
                                    [0, 0, 0, 0, 0, 0, 0, 0]],
                                lb=[-1, -1, 0, 0, 0, 0, 0, 0],
                                ub=[0, 0, 0, 0, 0, 0, 0, 0])

args = (x_obs, rot)
bounds = ((0, 1), (0, 1), (0, 150), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1))

results = differential_evolution(func=obj_func,
                                    bounds=bounds,
                                    constraints=constraints,
                                    args=args)

```

- `obj_func` has been described already

- `bounds` is a tuple of tuples, with each inner tuple
  containing the lower and upper bounds of a parameter.
  E.g., in our previous work we used `bounds = ((0, 1), (0,
  1), (0, 150), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1))`

- `constraints` is an object of class
  `scipy.optimize.LinearConstraint`. We spoke about this
  last time and it's a bit much to unpack right at the
  moment.

- `args` is a tuple of everything else `obj_func` needs to
  execute, but that isn't subjected to optimisation
  (`diferential_evolution()` won't change it to try to find
  the best fit).

- Note that there are many useful additional options that we
  might eventually specify, but we omit them now for
  simplicity.
